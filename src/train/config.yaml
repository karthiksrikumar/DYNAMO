model:
  base_model: "meta-llama/Meta-Llama-3-8B"
  time_embed_dim: 64
  causal_embed_dim: 128
  adapter_rank: 8
  dropout: 0.1

training:
  batch_size: 4
  learning_rate: 1e-4
  epochs: 5
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_seq_length: 256
  causal_alpha: 0.5

data:
  train_path: "data/train.jsonl"
  val_path: "data/val.jsonl"
  temporal_kb_path: "data/wikidata_temporal.parquet"
  causal_graph_path: "data/causal_graphs.parquet"
  max_samples: 100
